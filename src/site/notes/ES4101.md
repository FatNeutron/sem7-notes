---
{"dg-publish":true,"permalink":"/es-4101/","created":"2025-08-07T19:24:14.421+05:30","updated":"2025-08-11T16:09:42.141+05:30"}
---

# Marking Scheme

- **10 Marks:** 1 assignment
- **20 Marks:** 2 class tests; 1 after and 1 before midsem.
- **20 Marks:** midsem
- **50 Marks:** endsem

---

# Inverse Theory

**Inverse theory**, involves using mathematical techniques to solve problems where we only have the results or outcomes of a system, but not the details about how it works. It's like trying to figure out the cause of something by looking at its effects. In math, this often involves equations where we have to work backwards, starting with observed data, and then apply algorithms or methods to estimate the unknown factors. The goal is to find the best possible explanation or model that matches the data we have.

*Forward problem:*
model parameters ⟶ model ⟶prediction of data

*Inverse problem:*
data ⟶ model ⟶ estimate of model parameters

## Describing Inverse Problem

Let's assume a model with these linear equations,

$$
x_1 + x_2 = 1 \tag{1}
$$
$$
x_1 - x_2 = 2 \tag{2}
$$
we can express this in matrix form as, 

$$
\begin{bmatrix}
1 & 1 \\
1 & -1
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
=
\begin{bmatrix}
1 \\
2
\end{bmatrix}
\tag{3}
$$
 
We define matrices,

data: $d = \begin{bmatrix} d_1 \\ d_2 \\ d_3 \\ \vdots \\ d_N \end{bmatrix}$ , for $N$ number of data obtained.
model parameters: $m = \begin{bmatrix} m_1 \\ m_2 \\ m_3 \\ \vdots \\ m_M \end{bmatrix}$ , when $M$ number of info is available. 

so we can label the equation 3 as,

$$
G \cdot m = d \tag{5}
$$
i.e.,
$$
\underbrace{
\begin{bmatrix}
1 & 1 \\
1 & -1
\end{bmatrix}}_{G}
\underbrace{
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}}_{m}
=
\underbrace{
\begin{bmatrix}
1 \\
2
\end{bmatrix}}_{d}
\tag{6}
$$
Here $G$ is a relation matrix, which tell the relation between the unknown parameters and data. 

Based on the number of info available and number of unknown parameters, we can have 3 cases.

### Even determined problem

When the number of unknown parameters are **same** as the information available.

> [!Example]
> Assume a model with these equations,
> $$
> m_1 + m_2 = 1
> $$
> $$
> m_1 - m_2 = -1
> $$
> here number of unknown parameters($m_1,m_2$) and number of information(equations) are same. Hence a **Even determined problem**.

### Over-determined problem

When the number of unknown parameters are **less than** the number of information, it's a over-determined problem.

> [!Example]
> Assume a model with these equations,
> 
> $$
> m_1 + m_2 = 1
> $$
> $$
> m_1 - m_2 = -1
> $$
> $$
> 3m_1 + 2m_2 = 4
> $$
> $$
> 4m_1 - m_2 = 8
> $$
> here, number of unknown parameters(2) < number of equations(4), hence a **over-determined problem**.

### Under-determined problem

When the number of unknown parameters are **more than** the number of information, it's a under-determined problem.

> [!Example]
> Assume a model with these equations,
> 
> $$
> m_1 + 3m_2 - 2m_3 + 17m_4 = 69
> $$
> $$
> 6m_1 - 2m_2 - m_3 + m_4 = 13
> $$
> here, number of unknown parameters(4) > number of equations(2), hence a **under-determined problem**.

# Solving Over-determined problem

First let us define some term,

$d^{obs}_{i}$ : $i$th observed data
$d^{pre}_{i}$ : $i$th predicted data by the model

$e_i = |d^{obs}_{i} - d^{pre}_{i}|$ : prediction error
$E = \sum_{i=1}^{i=N} e^{2}_{i}$ : length norm

To graphically show these terms assume the model to of straight line of form $T_i = m_1 + m_2Z_i$,
![1.png](/img/user/ES4101-img/1.png)
$d_1,d_2, \dots, d_5$ are observed data, $d^{obs}_{i}$.

We need to minimize the error, $E$. Minimizing this problem mean just to reduce the sum of area of squares with length of $e_i$.
![2.png](/img/user/ES4101-img/2.png)
To do that we can $\frac{\partial{E}}{\partial{m}} = 0$. So,

$$
E = \sum_{i=1}^{N} e_i^2
$$
$$
E = \sum_{i=1}^{N} (d^{obs}_{i} - d^{pre}_{i})^2
$$
$$
E = \sum_{i=1}^{N} (d^{obs}_{i} - (m_1 + m_2Z_i))^2
$$
$$
E = \sum_{i=1}^{N} (d_i - m_1 - m_2Z_i)^2
$$
We need to minimize the error so,
$$
\frac{\partial{E}}{\partial{m_1}} =0= -2\sum_{i=1}^{N} (d_i - m_1 - m_2Z_i)
$$
$$
\sum_{i = 1}^N d_i = Nm_1 + m_2\sum Z_i
$$
also for $m_2$,
$$
\frac{\partial{E}}{\partial{m_2}} =0= -2\sum_{i=1}^{N}Z_i (d_i - m_1 - m_2Z_i)
$$
$$
\sum_{i=1}^{N} d_iZ_i - Nm_1\sum_{i=1}^{N}Z_i - m_2\sum_{i=1}^{N}Z_i^2 = 0
$$
$$
\sum_{i=1}^{N} d_iZ_i = Nm_1\sum_{i=1}^{N}Z_i + m_2\sum_{i=1}^{N}Z_i^2
$$
In the matrix format we can be represented as,

